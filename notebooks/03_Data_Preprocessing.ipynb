{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cbb3e5-8fd6-4c69-ba6c-c1e4a082ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9b7144-7e9d-4053-b59d-6799df309391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tiagolima/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7165fad-afd5-4b2e-8f45-b35ca6af297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:38:35.287445: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-29 13:38:35.315939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Número de GPUs disponíveis: 1\n",
      "Nome da GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "    \n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Verificar se há uma GPU disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Verificar a quantidade de GPUs disponíveis\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Número de GPUs disponíveis: {torch.cuda.device_count()}\")\n",
    "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7de644-ee36-4d56-be18-3a08e5868cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d052a638-b834-4265-9185-2fe64fddadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Carregar o tokenizador\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b000a8c-76e9-4d9e-a9ff-51d1d9674d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_files_dir = '../data/bio_json_data'\n",
    "bio_files = [os.path.join(bio_files_dir, f) for f in os.listdir('../data/bio_json_data') if f.endswith('.bio')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858a740f-9be2-4b1c-ae9e-c1ea25d6c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of .bio files is 285\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of .bio files is {len(bio_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48662cb8-27c4-44d1-8bae-62f227062c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import entity_to_acronyms, acronyms_to_entities, MAX_LENGTH #, label_to_index, index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b838080b-b7f9-48f3-9169-49881307c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bio_file in bio_files:\n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            \n",
    "            word, tag = line.strip().split('\\t')\n",
    "            if word in STOP_WORDS and tag.startswith('B'):\n",
    "                print(line)\n",
    "            # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727a45e7-4bc6-417c-a593-0af092e53aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    \n",
    "    # remove non-alphanumeric characters and extra whitespaces\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'\\s+',' ',word)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word not in STOP_WORDS:\n",
    "        return word\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f051cc-dac5-4e44-a2a3-a67d05a2fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Spacy model and set the stop words\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def clean_word(word):\n",
    "    # remove non-alphanumeric characters and extra whitespaces\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'\\s+',' ',word)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    word = word.lower()\n",
    "\n",
    "    try:\n",
    "        # lemmatize the word\n",
    "        lemma = nlp(word)[0].lemma_\n",
    "        \n",
    "        # check if the lemma is a stop word\n",
    "        if lemma not in STOP_WORDS:\n",
    "            return lemma\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5330dfa3-e9e7-4fe4-a59d-277079d799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_data_from_file(bio_file):\n",
    "    \"\"\"\n",
    "    Reads a file in BIO format (one token per line, with tab-separated word and tag),\n",
    "    and extracts the sentences and labels as lists of lists. Each inner list represents\n",
    "    a sentence, and contains the words of the sentence in order. Each corresponding inner\n",
    "    list in the 'labels' list contains the BIO tags for the words in the corresponding\n",
    "    sentence, in the same order.\n",
    "    \n",
    "    Args:\n",
    "    - bio_file (str): the path to the BioNLP file to read\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing:\n",
    "        - sentences (List[List[str]]): a list of lists, where each inner list represents\n",
    "          a sentence and contains the words of the sentence in order\n",
    "        - labels (List[List[str]]): a list of lists, where each inner list corresponds\n",
    "          to a sentence in the 'sentences' list and contains the BIO tags for the words\n",
    "          in the corresponding sentence, in the same order.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        current_sentences = []\n",
    "        current_labels = []\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            counter += 1\n",
    "            if line.strip() == '':\n",
    "                # If we encounter a blank line, it means we've reached the end of a sentence\n",
    "                if len(current_sentences) > 0:\n",
    "                    # print(current_sentences)\n",
    "                    \n",
    "                    # Add the current sentence and labels to the list\n",
    "                    sentences.append(current_sentences)\n",
    "                    labels.append(current_labels)\n",
    "                    \n",
    "                    # Reset the current sentence and labels lists\n",
    "                    current_sentences = []\n",
    "                    current_labels = []\n",
    "                    continue\n",
    "                    \n",
    "            word, tag = line.strip().split('\\t')\n",
    "            word = clean_word(word)\n",
    "            \n",
    "            if word.strip():\n",
    "                current_sentences.append(word)\n",
    "                # print(current_sentences)\n",
    "                if len(current_labels) > 0:\n",
    "                    if tag[2:] == current_labels[-1][2:] and tag[:2] == \"B-\":\n",
    "                        tag = f\"I-{tag[2:]}\"\n",
    "                current_labels.append(tag)\n",
    "\n",
    "        # print('counter', counter)\n",
    "        if counter > 0:\n",
    "            # print(len(current_sentences))\n",
    "            # Add the current sentence and labels to the list\n",
    "            sentences.append(current_sentences)\n",
    "            labels.append(current_labels)\n",
    "        \n",
    "            current_sentences = []\n",
    "            current_labels = []\n",
    "            \n",
    "    # print(sentences, labels)\n",
    "    # for (sentence, label) in zip(sentences, labels):\n",
    "    #     print(sentence, label)\n",
    "    #     print('*' * 50)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f568df22-7c9f-4bf3-9240-88fe645857eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bio_files(bio_files):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, bio_file in enumerate(bio_files):\n",
    "        \n",
    "        curr_sentences, curr_labels = parse_data_from_file(bio_file)\n",
    "        \n",
    "        if len(curr_sentences) > 0:\n",
    "            sentences.extend(curr_sentences)\n",
    "            labels.extend(curr_labels)\n",
    "            \n",
    "        # if (idx+1) % 20 == 0:\n",
    "        #     print(f'{idx+1} completed')\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6d7e4a-5e1f-4b1f-813f-8e96e359f09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m \u001b[43mparse_bio_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mparse_bio_files\u001b[0;34m(bio_files)\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, bio_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bio_files):\n\u001b[0;32m----> 8\u001b[0m     curr_sentences, curr_labels \u001b[38;5;241m=\u001b[39m \u001b[43mparse_data_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(curr_sentences) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m         sentences\u001b[38;5;241m.\u001b[39mextend(curr_sentences)\n",
      "Cell \u001b[0;32mIn[12], line 47\u001b[0m, in \u001b[0;36mparse_data_from_file\u001b[0;34m(bio_file)\u001b[0m\n\u001b[1;32m     44\u001b[0m         current_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m word, tag \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m word \u001b[38;5;241m=\u001b[39m clean_word(word)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstrip():\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "sentences, labels = parse_bio_files(bio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca641db-f7a1-430e-af94-a2e05ae71c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains {len(sentences)} examples\\n\")\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415f7cc-8931-4e3f-9dc7-f3098059c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f2f34-8719-4e84-ae04-2dc6e1d007c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {label: i for i, label in enumerate(labels)}\n",
    "# print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9ed80c-2ddf-42a9-8d52-672894607ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(sentences, labels))\n",
    "random.shuffle(combined)\n",
    "sentences[:], labels[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b51cd02e-9d86-4c5c-b6f6-3397562f1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento Sentences: 366\n",
      "Treinamento Labels: 366\n",
      "Teste Sentences: 92\n",
      "Teste Labels: 92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_labels = set(element for sublist in labels for element in sublist)\n",
    "label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "index_to_label = {id: label for label, id in label_to_index.items()}\n",
    "\n",
    "### Add the new label and ID to the dictionaries\n",
    "label_to_index['O'] = 0\n",
    "index_to_label[0] = 'O'\n",
    "\n",
    "# Função para substituir labels pelos índices\n",
    "def replace_labels_with_indices(labels, label_to_index):\n",
    "    return [[label_to_index[label] for label in sublist] for sublist in labels]\n",
    "\n",
    "# Substituir as labels pelos índices\n",
    "indexed_labels = replace_labels_with_indices(labels, label_to_index)\n",
    "# print(indexed_labels)\n",
    "# Empacotar sentenças e labels\n",
    "data = list(zip(sentences, indexed_labels))\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Desempacotar sentenças e labels\n",
    "train_sentences, train_labels = zip(*train_data)\n",
    "test_sentences, test_labels = zip(*test_data)\n",
    "\n",
    "# Converter de volta para listas\n",
    "train_sentences = list(train_sentences)\n",
    "train_labels = list(train_labels)\n",
    "test_sentences = list(test_sentences)\n",
    "test_labels = list(test_labels)\n",
    "\n",
    "# Exibir os conjuntos de treino e teste\n",
    "print(\"Treinamento Sentences:\", len(train_sentences))\n",
    "print(\"Treinamento Labels:\", len(train_labels))\n",
    "print(\"Teste Sentences:\", len(test_sentences))\n",
    "print(\"Teste Labels:\", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e5889f-2d61-4f4c-84d7-ba5775141707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 08:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.639681</td>\n",
       "      <td>0.368762</td>\n",
       "      <td>0.245219</td>\n",
       "      <td>0.368762</td>\n",
       "      <td>0.259557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.131641</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.372857</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.361943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.931820</td>\n",
       "      <td>0.506675</td>\n",
       "      <td>0.475987</td>\n",
       "      <td>0.506675</td>\n",
       "      <td>0.445913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.789081</td>\n",
       "      <td>0.546468</td>\n",
       "      <td>0.510608</td>\n",
       "      <td>0.546468</td>\n",
       "      <td>0.496870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.667607</td>\n",
       "      <td>0.593001</td>\n",
       "      <td>0.553339</td>\n",
       "      <td>0.593001</td>\n",
       "      <td>0.556776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.979700</td>\n",
       "      <td>1.654075</td>\n",
       "      <td>0.605962</td>\n",
       "      <td>0.574363</td>\n",
       "      <td>0.605962</td>\n",
       "      <td>0.575160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.979700</td>\n",
       "      <td>1.639859</td>\n",
       "      <td>0.614388</td>\n",
       "      <td>0.575999</td>\n",
       "      <td>0.614388</td>\n",
       "      <td>0.584686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.979700</td>\n",
       "      <td>1.647853</td>\n",
       "      <td>0.616202</td>\n",
       "      <td>0.590342</td>\n",
       "      <td>0.616202</td>\n",
       "      <td>0.587732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.979700</td>\n",
       "      <td>1.696080</td>\n",
       "      <td>0.620220</td>\n",
       "      <td>0.588117</td>\n",
       "      <td>0.620220</td>\n",
       "      <td>0.592255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.979700</td>\n",
       "      <td>1.685435</td>\n",
       "      <td>0.617369</td>\n",
       "      <td>0.588278</td>\n",
       "      <td>0.617369</td>\n",
       "      <td>0.594199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.712443</td>\n",
       "      <td>0.630201</td>\n",
       "      <td>0.619497</td>\n",
       "      <td>0.630201</td>\n",
       "      <td>0.612585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.760910</td>\n",
       "      <td>0.623590</td>\n",
       "      <td>0.605060</td>\n",
       "      <td>0.623590</td>\n",
       "      <td>0.605061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.806451</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0.613850</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0.609815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.822048</td>\n",
       "      <td>0.626831</td>\n",
       "      <td>0.613808</td>\n",
       "      <td>0.626831</td>\n",
       "      <td>0.610067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.853989</td>\n",
       "      <td>0.632404</td>\n",
       "      <td>0.619230</td>\n",
       "      <td>0.632404</td>\n",
       "      <td>0.613265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.903256</td>\n",
       "      <td>0.632534</td>\n",
       "      <td>0.620961</td>\n",
       "      <td>0.632534</td>\n",
       "      <td>0.618836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>1.936969</td>\n",
       "      <td>0.627609</td>\n",
       "      <td>0.617852</td>\n",
       "      <td>0.627609</td>\n",
       "      <td>0.612543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>1.947337</td>\n",
       "      <td>0.631627</td>\n",
       "      <td>0.621844</td>\n",
       "      <td>0.631627</td>\n",
       "      <td>0.618934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>1.986067</td>\n",
       "      <td>0.624627</td>\n",
       "      <td>0.619055</td>\n",
       "      <td>0.624627</td>\n",
       "      <td>0.615478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>1.992362</td>\n",
       "      <td>0.636682</td>\n",
       "      <td>0.626128</td>\n",
       "      <td>0.636682</td>\n",
       "      <td>0.624934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>2.016945</td>\n",
       "      <td>0.634738</td>\n",
       "      <td>0.624395</td>\n",
       "      <td>0.634738</td>\n",
       "      <td>0.623766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.029812</td>\n",
       "      <td>0.634089</td>\n",
       "      <td>0.626954</td>\n",
       "      <td>0.634089</td>\n",
       "      <td>0.623050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.061110</td>\n",
       "      <td>0.638108</td>\n",
       "      <td>0.627049</td>\n",
       "      <td>0.638108</td>\n",
       "      <td>0.627106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.084479</td>\n",
       "      <td>0.633830</td>\n",
       "      <td>0.624087</td>\n",
       "      <td>0.633830</td>\n",
       "      <td>0.623642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.101940</td>\n",
       "      <td>0.635386</td>\n",
       "      <td>0.626683</td>\n",
       "      <td>0.635386</td>\n",
       "      <td>0.625335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.123507</td>\n",
       "      <td>0.628516</td>\n",
       "      <td>0.621041</td>\n",
       "      <td>0.628516</td>\n",
       "      <td>0.618956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>2.116838</td>\n",
       "      <td>0.633052</td>\n",
       "      <td>0.625178</td>\n",
       "      <td>0.633052</td>\n",
       "      <td>0.623812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>2.127110</td>\n",
       "      <td>0.632275</td>\n",
       "      <td>0.622467</td>\n",
       "      <td>0.632275</td>\n",
       "      <td>0.621977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>2.145147</td>\n",
       "      <td>0.629942</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.629942</td>\n",
       "      <td>0.619912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>2.148680</td>\n",
       "      <td>0.632145</td>\n",
       "      <td>0.622953</td>\n",
       "      <td>0.632145</td>\n",
       "      <td>0.621424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tiagolima/anaconda3/envs/pybot/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados da Avaliação: {'eval_loss': 2.1486799716949463, 'eval_accuracy': 0.632145171743357, 'eval_precision': 0.6229526048658711, 'eval_recall': 0.632145171743357, 'eval_f1': 0.6214238202766369, 'eval_runtime': 0.6373, 'eval_samples_per_second': 144.358, 'eval_steps_per_second': 36.09, 'epoch': 30.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Carregar o tokenizer BERT pré-treinado\n",
    "# tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# Carregar o tokenizer rápido BERT pré-treinado para português\n",
    "tokenizer = BertTokenizerFast.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "# tokenizer.to(device)  # Mover o modelo para a GPU\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(sentences, labels, tokenizer, max_length=128):\n",
    "    tokenized_inputs = tokenizer(sentences, padding='max_length', truncation=True, max_length=max_length, is_split_into_words=True)\n",
    "    new_labels = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        new_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenizar e alinhar labels para treino e teste\n",
    "train_inputs = tokenize_and_align_labels(train_sentences, train_labels, tokenizer, max_length=128)\n",
    "test_inputs = tokenize_and_align_labels(test_sentences, test_labels, tokenizer, max_length=128)\n",
    "\n",
    "# Converter para dataset do Hugging Face\n",
    "train_dataset = Dataset.from_dict(train_inputs)\n",
    "test_dataset = Dataset.from_dict(test_inputs)\n",
    "\n",
    "# Carregar o modelo BERT pré-treinado para classificação de tokens\n",
    "model = BertForTokenClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=len(set(label for sublist in labels for label in sublist)))\n",
    "model.to(device)  # Mover o modelo para a GPU\n",
    "\n",
    "# Definir a função de métricas\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=-1)\n",
    "    labels = p.label_ids\n",
    "    # Remover os rótulos de preenchimento para calcular métricas corretamente\n",
    "    true_predictions = [\n",
    "        [pred for pred, label in zip(prediction, label) if label != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label for label in label_list if label != -100]\n",
    "        for label_list in labels\n",
    "    ]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        [l for sublist in true_labels for l in sublist],\n",
    "        [p for sublist in true_predictions for p in sublist],\n",
    "        average='weighted'\n",
    "    )\n",
    "    acc = accuracy_score(\n",
    "        [l for sublist in true_labels for l in sublist],\n",
    "        [p for sublist in true_predictions for p in sublist]\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Configurar os parâmetros de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_v3\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.03, #0.01\n",
    ")\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Avaliar o modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"Resultados da Avaliação:\", results)\n",
    "\n",
    "trainer.save_model(\"./my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8f454-bdf5-4c49-a7fd-3f22db87a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tokenizar e preparar o texto\n",
    "def tokenize_and_prepare_texts(texts, tokenizer, max_length=128):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d3394-e255-436e-83e8-cdb4dd3c6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo texto para validação\n",
    "\n",
    "# new_text = [\"Acesso venoso central em subclavia D duplolumen recebendo solução salina e glicosada em BI.\"]\n",
    "new_text = [\"Paciente com Sepse pulmonar em D8 tazocin (paciente não recebeu por 2 dias Atb).\"]\n",
    "\n",
    "# new_text = [\"#exames laboratoriais lab (03/03/2021) hb 9.09/ ht 24.7/ leuco 12800/ plaq 131.000/ pcr 370/ su nitrito negativo, leuco 15 20/cp, 1 2 hemacias varias celulas epiteliais, varias bacterias #.\"]\n",
    "new_text = [ \"admissao paciente 16 anos, g1p0, admitida dia 21/12/21, no curso de 31.2 sem, encaminhada de gravata devido historia de perda de liquido amniotico claro desde as 11h do dia 21/12. negava comorbidades relatava boa movimentacao fetal apresenta registro em encaminhamento de realizacao de 1 dose de dexametasona em servico de origem as 16:40h.\"]\n",
    "# Tokenizar o novo texto\n",
    "tokenized_inputs = tokenize_and_prepare_texts(new_text, tokenizer)\n",
    "\n",
    "# Mover os tensores para a GPU se disponível\n",
    "tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08e75637-81ad-43b1-b14d-08dd5ac17e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admissao: O\n",
      "paciente: B-SUB\n",
      "16: B-AGE\n",
      "anos: I-AGE\n",
      ",: O\n",
      "g1p0: B-PREH\n",
      ",: O\n",
      "admitida: B-CLE\n",
      "dia: O\n",
      "21: B-TIM\n",
      "/: B-TIM\n",
      "12: B-TIM\n",
      "/: B-TIM\n",
      "21: B-TIM\n",
      ",: O\n",
      "no: O\n",
      "curso: O\n",
      "de: O\n",
      "31: O\n",
      ".: O\n",
      "2: O\n",
      "sem: O\n",
      ",: O\n",
      "encaminhada: B-CLE\n",
      "de: B-DET\n",
      "gravata: I-DET\n",
      "devido: B-DET\n",
      "historia: B-DID\n",
      "de: B-DID\n",
      "perda: B-BAT\n",
      "de: B-BAT\n",
      "liquido: B-BAT\n",
      "amniotico: B-COL\n",
      "claro: I-BAT\n",
      "desde: O\n",
      "as: B-TIM\n",
      "11h: B-TIM\n",
      "do: B-TIM\n",
      "dia: B-TIM\n",
      "21: B-TIM\n",
      "/: B-TIM\n",
      "12: B-TIM\n",
      ".: O\n",
      "negava: B-SIG\n",
      "comorbidades: I-SIG\n",
      "relatava: B-SIG\n",
      "boa: B-SIG\n",
      "movimentacao: I-SIG\n",
      "fetal: I-SIG\n",
      "apresenta: O\n",
      "registro: O\n",
      "em: O\n",
      "encaminhamento: I-DET\n",
      "de: I-DET\n",
      "realizacao: I-DET\n",
      "de: O\n",
      "1: B-DOS\n",
      "dose: I-DOS\n",
      "de: B-MED\n",
      "dexametasona: B-MED\n",
      "em: O\n",
      "servico: I-HIS\n",
      "de: I-DET\n",
      "origem: I-HIS\n",
      "as: O\n",
      "16: B-TIM\n",
      ":: B-TIM\n",
      "40h: B-TIM\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_inputs)\n",
    "\n",
    "# Obter as previsões\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Inverter o dicionário de mapeamento para obter rótulos a partir dos índices\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "# Obter as palavras do novo texto tokenizado\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])\n",
    "\n",
    "# Mapear as previsões para os rótulos\n",
    "predicted_labels = [index_to_label[idx.item()] for idx in predictions[0]]\n",
    "\n",
    "# Remover tokens especiais ([CLS], [SEP], [PAD])\n",
    "tokens_labels = [(token, label) for token, label in zip(tokens, predicted_labels) if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n",
    "\n",
    "# Reconstruir palavras a partir dos tokens (tokens BERT podem ser subpalavras)\n",
    "word_labels = []\n",
    "current_word = \"\"\n",
    "current_label = \"\"\n",
    "for token, label in tokens_labels:\n",
    "    if token.startswith(\"##\"):\n",
    "        current_word += token[2:]\n",
    "    else:\n",
    "        if current_word:\n",
    "            word_labels.append((current_word, current_label))\n",
    "        current_word = token\n",
    "        current_label = label\n",
    "if current_word:\n",
    "    word_labels.append((current_word, current_label))\n",
    "\n",
    "# Exibir as palavras do texto com suas respectivas previsões\n",
    "for word, label in word_labels:\n",
    "    print(f\"{word}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7697e8a-5c04-4db5-805c-54d9ba365f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_index = np.arange(0, len(sentences))\n",
    "labels_index = np.arange(0, len(labels))\n",
    "\n",
    "# sentences = np.array(sentences)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "\n",
    "# sss_train_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# train_index, val_test_index = next(sss_train_val_test.split(sentences, labels))\n",
    "\n",
    "\n",
    "# print(train_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Primeiro, dividir em treinamento e (validação + teste)\n",
    "# sss_train_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# train_index, val_test_index = next(sss_train_val_test.split(sentences, labels))\n",
    "\n",
    "# X_train, X_val_test = sentences[train_index], sentences[val_test_index]\n",
    "# y_train, y_val_test = labels[train_index], labels[val_test_index]\n",
    "\n",
    "# # Em seguida, dividir (validação + teste) em validação e teste\n",
    "# sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "# val_index, test_index = next(sss_val_test.split(X_val_test, y_val_test))\n",
    "\n",
    "# X_val, X_test = X_val_test[val_index], X_val_test[test_index]\n",
    "# y_val, y_test = y_val_test[val_index], y_val_test[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efd88e4c-8550-4806-889e-dbabf6b4c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "num_train = int(num_sentences * (1 - TEST_SIZE - 0.1))\n",
    "num_valid = int(num_sentences * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35d7b7a8-ed8b-4fdc-a63a-3da8eae7d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:num_train]\n",
    "train_labels = labels[:num_train]\n",
    "\n",
    "valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "valid_labels = labels[num_train:num_train+num_valid]\n",
    "\n",
    "test_sentences = sentences[num_train+num_valid:]\n",
    "test_labels = labels[num_train+num_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cd18843-5d9a-4667-b06c-cd85f07118a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(element for sublist in labels for element in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f32efdf-5237-4fda-8159-d0032c31d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "index_to_label = {id: label for label, id in label_to_index.items()}\n",
    "\n",
    "### Add the new label and ID to the dictionaries\n",
    "label_to_index['<PAD>'] = 0\n",
    "index_to_label[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21d7507a-17eb-4651-b37e-9f234da7dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-SUB', 'B-BAT', 'B-HEI', 'I-PREG', 'I-QLTC', 'B-HIS', 'I-ARA', 'B-THP', 'B-PREH', 'I-TIM', 'I-NBL', 'B-OUT', 'B-OTE', 'B-FAM', 'B-DOS', 'B-BST', 'B-QLTC', 'I-ACT', 'B-ADM', 'I-WEI', 'I-AGE', 'B-PER', 'B-DID', 'I-DOS', 'I-ADM', 'I-MED', 'B-ACT', 'B-WEI', 'B-MED', 'I-PREH', 'B-SEX', 'I-DIS', 'B-OCC', 'I-OTE', 'I-QNTC', 'B-VOL', 'I-DET', 'I-VOL', 'I-DUR', 'I-SEV', 'B-OTH', 'I-MEDT', 'B-FRE', 'B-NBL', 'B-QNTC', 'B-MEDT', 'I-SIG', 'B-DIA', 'B-LAB', 'B-TIM', 'I-HEI', 'B-COL', 'I-OCC', 'B-DIS', 'I-FAM', 'B-DUR', 'I-COL', 'I-SUB', 'I-BAT', 'I-HIS', 'I-LAB', 'I-CLE', 'B-SEV', 'I-DIA', 'B-AGE', 'I-OTH', 'I-PER', 'B-SIG', 'B-CLE', 'I-DID', 'B-ARA', 'I-BST', 'I-FRE', 'I-DAT', 'I-OUT', 'O', 'B-DAT', 'B-DET', 'B-PREG', 'I-THP'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11fec2b4-8fd0-4875-9c23-9fa279ed6e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ACT': 1, 'B-ADM': 2, 'B-AGE': 3, 'B-ARA': 4, 'B-BAT': 5, 'B-BST': 6, 'B-CLE': 7, 'B-COL': 8, 'B-DAT': 9, 'B-DET': 10, 'B-DIA': 11, 'B-DID': 12, 'B-DIS': 13, 'B-DOS': 14, 'B-DUR': 15, 'B-FAM': 16, 'B-FRE': 17, 'B-HEI': 18, 'B-HIS': 19, 'B-LAB': 20, 'B-MED': 21, 'B-MEDT': 22, 'B-NBL': 23, 'B-OCC': 24, 'B-OTE': 25, 'B-OTH': 26, 'B-OUT': 27, 'B-PER': 28, 'B-PREG': 29, 'B-PREH': 30, 'B-QLTC': 31, 'B-QNTC': 32, 'B-SEV': 33, 'B-SEX': 34, 'B-SIG': 35, 'B-SUB': 36, 'B-THP': 37, 'B-TIM': 38, 'B-VOL': 39, 'B-WEI': 40, 'I-ACT': 41, 'I-ADM': 42, 'I-AGE': 43, 'I-ARA': 44, 'I-BAT': 45, 'I-BST': 46, 'I-CLE': 47, 'I-COL': 48, 'I-DAT': 49, 'I-DET': 50, 'I-DIA': 51, 'I-DID': 52, 'I-DIS': 53, 'I-DOS': 54, 'I-DUR': 55, 'I-FAM': 56, 'I-FRE': 57, 'I-HEI': 58, 'I-HIS': 59, 'I-LAB': 60, 'I-MED': 61, 'I-MEDT': 62, 'I-NBL': 63, 'I-OCC': 64, 'I-OTE': 65, 'I-OTH': 66, 'I-OUT': 67, 'I-PER': 68, 'I-PREG': 69, 'I-PREH': 70, 'I-QLTC': 71, 'I-QNTC': 72, 'I-SEV': 73, 'I-SIG': 74, 'I-SUB': 75, 'I-THP': 76, 'I-TIM': 77, 'I-VOL': 78, 'I-WEI': 79, 'O': 80, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_index)\n",
    "# label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "# index_to_label = {id: label for label, id in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d434727d-5a0a-4a3a-8b20-794ad2ddab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'B-ACT', 2: 'B-ADM', 3: 'B-AGE', 4: 'B-ARA', 5: 'B-BAT', 6: 'B-BST', 7: 'B-CLE', 8: 'B-COL', 9: 'B-DAT', 10: 'B-DET', 11: 'B-DIA', 12: 'B-DID', 13: 'B-DIS', 14: 'B-DOS', 15: 'B-DUR', 16: 'B-FAM', 17: 'B-FRE', 18: 'B-HEI', 19: 'B-HIS', 20: 'B-LAB', 21: 'B-MED', 22: 'B-MEDT', 23: 'B-NBL', 24: 'B-OCC', 25: 'B-OTE', 26: 'B-OTH', 27: 'B-OUT', 28: 'B-PER', 29: 'B-PREG', 30: 'B-PREH', 31: 'B-QLTC', 32: 'B-QNTC', 33: 'B-SEV', 34: 'B-SEX', 35: 'B-SIG', 36: 'B-SUB', 37: 'B-THP', 38: 'B-TIM', 39: 'B-VOL', 40: 'B-WEI', 41: 'I-ACT', 42: 'I-ADM', 43: 'I-AGE', 44: 'I-ARA', 45: 'I-BAT', 46: 'I-BST', 47: 'I-CLE', 48: 'I-COL', 49: 'I-DAT', 50: 'I-DET', 51: 'I-DIA', 52: 'I-DID', 53: 'I-DIS', 54: 'I-DOS', 55: 'I-DUR', 56: 'I-FAM', 57: 'I-FRE', 58: 'I-HEI', 59: 'I-HIS', 60: 'I-LAB', 61: 'I-MED', 62: 'I-MEDT', 63: 'I-NBL', 64: 'I-OCC', 65: 'I-OTE', 66: 'I-OTH', 67: 'I-OUT', 68: 'I-PER', 69: 'I-PREG', 70: 'I-PREH', 71: 'I-QLTC', 72: 'I-QNTC', 73: 'I-SEV', 74: 'I-SIG', 75: 'I-SUB', 76: 'I-THP', 77: 'I-TIM', 78: 'I-VOL', 79: 'I-WEI', 80: 'O', 0: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_label)\n",
    "# # Add the new label and ID to the dictionaries\n",
    "# label_to_index['<PAD>'] = 0\n",
    "# index_to_label[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c64dd531-81e7-48c8-9583-5a7ced7743f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(index_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f2cb995-3bb1-44e3-bc69-e3e46d91174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "train_labels = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "train_labels = pad_sequences(train_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "train_labels = to_categorical(train_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "valid_labels = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "valid_labels = pad_sequences(valid_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "test_labels = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "test_labels = pad_sequences(test_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "test_labels = to_categorical(test_labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06629caf-188b-4bdb-a071-41b078260768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfc1fb72-76cf-4943-ac6e-5f1c22d81b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    '../data/data.npz',\n",
    "     train_sequences_padded=train_sequences_padded,\n",
    "     train_labels=train_labels,\n",
    "     val_sequences_padded=val_sequences_padded,\n",
    "     val_labels=valid_labels,\n",
    "     test_sequences_padded=test_sequences_padded,\n",
    "     test_labels=test_labels,\n",
    "     label_to_index=label_to_index,\n",
    "     index_to_label=index_to_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36eb3093-d4af-4911-9fc8-eb533c000643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 100, 81)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1a49661-c8ba-47ed-a34e-d6d312cb68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo BERT pré-treinado para token classification\n",
    "# model = TFBertForTokenClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=len(label_list))\n",
    "\n",
    "# # Compilar o modelo\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# # Treinar o modelo\n",
    "# model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n",
    "# # Salvar o modelo treinado\n",
    "# model.save_pretrained('./fine_tuned_ner_model')\n",
    "# tokenizer.save_pretrained('./fine_tuned_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b904cf1-d07a-476d-99b6-d5a331cc1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(tokenizer.word_index)+1\n",
    "EMBEDDING_DIM = 216\n",
    "NUM_CLASSES = len(label_to_index)\n",
    "MAX_LENGTH = train_sequences_padded.shape[1]\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44032476-6f5c-422c-b3b3-9da1a53e0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    _precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return _precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute recall metric\"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Compute f1-score metric\"\"\"\n",
    "    _precision = precision(y_true, y_pred)\n",
    "    _recall = recall(y_true, y_pred)\n",
    "    f1_score = 2 * ((_precision * _recall) / (_precision + _recall + K.epsilon()))\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9f15484-1b35-49ff-9f3a-21897349fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:59:35.760789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-17 14:59:35.761218: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-17 14:59:35.905986: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:35.906596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:35.907150: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:35.968358: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 216)          552096    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         143872    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100, 79)           10191     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 706,159\n",
      "Trainable params: 706,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:59:35.991175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:35.991903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:35.992620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    Embedding(INPUT_DIM, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ec44f72-ad4e-48b7-90bb-07318b00a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:59:45.051456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:45.052263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:45.052814: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:45.114673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-06-17 14:59:45.136813: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:45.137513: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:45.138147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:45.286347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-06-17 14:59:45.555758: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:45.556389: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:45.557114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:45.617228: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-06-17 14:59:45.638386: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:45.639050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:45.639633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:45.784901: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 3.9511 - accuracy: 0.6768 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:59:46.571361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:46.572202: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:46.572775: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-06-17 14:59:46.633645: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-06-17 14:59:46.654589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-06-17 14:59:46.655233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-06-17 14:59:46.655781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 67ms/step - loss: 3.9511 - accuracy: 0.6768 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 2.9508 - val_accuracy: 0.7782 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.8909 - accuracy: 0.7606 - precision: 0.5716 - recall: 0.3738 - f1_score: 0.4476 - val_loss: 1.2793 - val_accuracy: 0.7782 - val_precision: 0.8416 - val_recall: 0.7534 - val_f1_score: 0.7951\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.3063 - accuracy: 0.7606 - precision: 0.8357 - recall: 0.7379 - f1_score: 0.7833 - val_loss: 1.0736 - val_accuracy: 0.7782 - val_precision: 0.9400 - val_recall: 0.7510 - val_f1_score: 0.8349\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 1.1012 - accuracy: 0.7588 - precision: 0.9827 - recall: 0.7030 - f1_score: 0.8193 - val_loss: 0.9932 - val_accuracy: 0.7782 - val_precision: 0.9688 - val_recall: 0.7393 - val_f1_score: 0.8386\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.0366 - accuracy: 0.7596 - precision: 0.9982 - recall: 0.6960 - f1_score: 0.8196 - val_loss: 0.9620 - val_accuracy: 0.7782 - val_precision: 0.9647 - val_recall: 0.7411 - val_f1_score: 0.8382\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9832 - accuracy: 0.7605 - precision: 0.9902 - recall: 0.7061 - f1_score: 0.8233 - val_loss: 0.9608 - val_accuracy: 0.7782 - val_precision: 0.9543 - val_recall: 0.7444 - val_f1_score: 0.8364\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.9586 - accuracy: 0.7606 - precision: 0.9851 - recall: 0.7133 - f1_score: 0.8263 - val_loss: 0.9598 - val_accuracy: 0.7782 - val_precision: 0.9551 - val_recall: 0.7449 - val_f1_score: 0.8370\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.9439 - accuracy: 0.7615 - precision: 0.9902 - recall: 0.7120 - f1_score: 0.8281 - val_loss: 0.9570 - val_accuracy: 0.7809 - val_precision: 0.9575 - val_recall: 0.7438 - val_f1_score: 0.8372\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.9334 - accuracy: 0.7664 - precision: 0.9926 - recall: 0.7110 - f1_score: 0.8277 - val_loss: 0.9577 - val_accuracy: 0.7836 - val_precision: 0.9566 - val_recall: 0.7441 - val_f1_score: 0.8371\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.9244 - accuracy: 0.7682 - precision: 0.9904 - recall: 0.7129 - f1_score: 0.8286 - val_loss: 0.9613 - val_accuracy: 0.7858 - val_precision: 0.9522 - val_recall: 0.7461 - val_f1_score: 0.8366\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.9160 - accuracy: 0.7689 - precision: 0.9894 - recall: 0.7166 - f1_score: 0.8307 - val_loss: 0.9585 - val_accuracy: 0.7862 - val_precision: 0.9523 - val_recall: 0.7472 - val_f1_score: 0.8374\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9064 - accuracy: 0.7710 - precision: 0.9902 - recall: 0.7176 - f1_score: 0.8314 - val_loss: 0.9553 - val_accuracy: 0.7871 - val_precision: 0.9517 - val_recall: 0.7474 - val_f1_score: 0.8373\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8967 - accuracy: 0.7721 - precision: 0.9895 - recall: 0.7195 - f1_score: 0.8320 - val_loss: 0.9538 - val_accuracy: 0.7871 - val_precision: 0.9518 - val_recall: 0.7479 - val_f1_score: 0.8377\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8859 - accuracy: 0.7738 - precision: 0.9902 - recall: 0.7209 - f1_score: 0.8341 - val_loss: 0.9525 - val_accuracy: 0.7878 - val_precision: 0.9520 - val_recall: 0.7485 - val_f1_score: 0.8381\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8733 - accuracy: 0.7755 - precision: 0.9906 - recall: 0.7224 - f1_score: 0.8352 - val_loss: 0.9461 - val_accuracy: 0.7887 - val_precision: 0.9526 - val_recall: 0.7485 - val_f1_score: 0.8383\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8592 - accuracy: 0.7770 - precision: 0.9898 - recall: 0.7244 - f1_score: 0.8359 - val_loss: 0.9414 - val_accuracy: 0.7884 - val_precision: 0.9520 - val_recall: 0.7488 - val_f1_score: 0.8382\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8441 - accuracy: 0.7793 - precision: 0.9908 - recall: 0.7247 - f1_score: 0.8363 - val_loss: 0.9333 - val_accuracy: 0.7907 - val_precision: 0.9517 - val_recall: 0.7491 - val_f1_score: 0.8383\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.8249 - accuracy: 0.7838 - precision: 0.9900 - recall: 0.7268 - f1_score: 0.8374 - val_loss: 0.9251 - val_accuracy: 0.7929 - val_precision: 0.9524 - val_recall: 0.7494 - val_f1_score: 0.8388\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.8038 - accuracy: 0.7910 - precision: 0.9899 - recall: 0.7283 - f1_score: 0.8387 - val_loss: 0.9153 - val_accuracy: 0.7984 - val_precision: 0.9504 - val_recall: 0.7506 - val_f1_score: 0.8388\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.7771 - accuracy: 0.8007 - precision: 0.9898 - recall: 0.7307 - f1_score: 0.8405 - val_loss: 0.9051 - val_accuracy: 0.8024 - val_precision: 0.9502 - val_recall: 0.7514 - val_f1_score: 0.8391\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.7500 - accuracy: 0.8063 - precision: 0.9893 - recall: 0.7340 - f1_score: 0.8425 - val_loss: 0.8928 - val_accuracy: 0.8069 - val_precision: 0.9491 - val_recall: 0.7538 - val_f1_score: 0.8402\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.7227 - accuracy: 0.8112 - precision: 0.9871 - recall: 0.7389 - f1_score: 0.8444 - val_loss: 0.8847 - val_accuracy: 0.8073 - val_precision: 0.9459 - val_recall: 0.7545 - val_f1_score: 0.8394\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.6972 - accuracy: 0.8172 - precision: 0.9873 - recall: 0.7416 - f1_score: 0.8462 - val_loss: 0.8799 - val_accuracy: 0.8100 - val_precision: 0.9424 - val_recall: 0.7579 - val_f1_score: 0.8401\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.6728 - accuracy: 0.8197 - precision: 0.9860 - recall: 0.7454 - f1_score: 0.8485 - val_loss: 0.8781 - val_accuracy: 0.8124 - val_precision: 0.9426 - val_recall: 0.7600 - val_f1_score: 0.8415\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.6478 - accuracy: 0.8263 - precision: 0.9862 - recall: 0.7467 - f1_score: 0.8496 - val_loss: 0.8687 - val_accuracy: 0.8136 - val_precision: 0.9421 - val_recall: 0.7613 - val_f1_score: 0.8421\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.6244 - accuracy: 0.8313 - precision: 0.9846 - recall: 0.7524 - f1_score: 0.8524 - val_loss: 0.8560 - val_accuracy: 0.8151 - val_precision: 0.9413 - val_recall: 0.7638 - val_f1_score: 0.8433\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.6017 - accuracy: 0.8378 - precision: 0.9838 - recall: 0.7559 - f1_score: 0.8544 - val_loss: 0.8514 - val_accuracy: 0.8176 - val_precision: 0.9406 - val_recall: 0.7678 - val_f1_score: 0.8454\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.5802 - accuracy: 0.8451 - precision: 0.9831 - recall: 0.7600 - f1_score: 0.8569 - val_loss: 0.8548 - val_accuracy: 0.8187 - val_precision: 0.9377 - val_recall: 0.7692 - val_f1_score: 0.8451\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.5588 - accuracy: 0.8506 - precision: 0.9830 - recall: 0.7649 - f1_score: 0.8601 - val_loss: 0.8511 - val_accuracy: 0.8207 - val_precision: 0.9350 - val_recall: 0.7743 - val_f1_score: 0.8471\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.5389 - accuracy: 0.8581 - precision: 0.9806 - recall: 0.7715 - f1_score: 0.8634 - val_loss: 0.8506 - val_accuracy: 0.8240 - val_precision: 0.9340 - val_recall: 0.7781 - val_f1_score: 0.8490\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.5185 - accuracy: 0.8659 - precision: 0.9801 - recall: 0.7787 - f1_score: 0.8675 - val_loss: 0.8531 - val_accuracy: 0.8256 - val_precision: 0.9293 - val_recall: 0.7832 - val_f1_score: 0.8501\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.4999 - accuracy: 0.8700 - precision: 0.9784 - recall: 0.7883 - f1_score: 0.8729 - val_loss: 0.8528 - val_accuracy: 0.8269 - val_precision: 0.9296 - val_recall: 0.7832 - val_f1_score: 0.8501\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.4820 - accuracy: 0.8776 - precision: 0.9783 - recall: 0.7939 - f1_score: 0.8764 - val_loss: 0.8537 - val_accuracy: 0.8278 - val_precision: 0.9234 - val_recall: 0.7879 - val_f1_score: 0.8503\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.4641 - accuracy: 0.8816 - precision: 0.9771 - recall: 0.8034 - f1_score: 0.8816 - val_loss: 0.8568 - val_accuracy: 0.8284 - val_precision: 0.9183 - val_recall: 0.7928 - val_f1_score: 0.8509\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.4471 - accuracy: 0.8857 - precision: 0.9758 - recall: 0.8116 - f1_score: 0.8861 - val_loss: 0.8626 - val_accuracy: 0.8284 - val_precision: 0.9159 - val_recall: 0.7965 - val_f1_score: 0.8520\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.4295 - accuracy: 0.8916 - precision: 0.9758 - recall: 0.8184 - f1_score: 0.8900 - val_loss: 0.8662 - val_accuracy: 0.8293 - val_precision: 0.9127 - val_recall: 0.7988 - val_f1_score: 0.8520\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.4136 - accuracy: 0.8954 - precision: 0.9743 - recall: 0.8263 - f1_score: 0.8940 - val_loss: 0.8657 - val_accuracy: 0.8287 - val_precision: 0.9125 - val_recall: 0.7998 - val_f1_score: 0.8524\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.3991 - accuracy: 0.8984 - precision: 0.9739 - recall: 0.8329 - f1_score: 0.8977 - val_loss: 0.8686 - val_accuracy: 0.8289 - val_precision: 0.9090 - val_recall: 0.8024 - val_f1_score: 0.8524\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.3835 - accuracy: 0.9026 - precision: 0.9741 - recall: 0.8395 - f1_score: 0.9015 - val_loss: 0.8698 - val_accuracy: 0.8287 - val_precision: 0.9074 - val_recall: 0.8037 - val_f1_score: 0.8524\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.3711 - accuracy: 0.9053 - precision: 0.9729 - recall: 0.8464 - f1_score: 0.9051 - val_loss: 0.8816 - val_accuracy: 0.8287 - val_precision: 0.9032 - val_recall: 0.8075 - val_f1_score: 0.8526\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.3578 - accuracy: 0.9097 - precision: 0.9734 - recall: 0.8520 - f1_score: 0.9085 - val_loss: 0.8849 - val_accuracy: 0.8298 - val_precision: 0.9031 - val_recall: 0.8079 - val_f1_score: 0.8529\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.3476 - accuracy: 0.9118 - precision: 0.9735 - recall: 0.8576 - f1_score: 0.9118 - val_loss: 0.8963 - val_accuracy: 0.8287 - val_precision: 0.8995 - val_recall: 0.8109 - val_f1_score: 0.8529\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.3347 - accuracy: 0.9143 - precision: 0.9732 - recall: 0.8623 - f1_score: 0.9143 - val_loss: 0.9045 - val_accuracy: 0.8287 - val_precision: 0.8969 - val_recall: 0.8127 - val_f1_score: 0.8527\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.3238 - accuracy: 0.9175 - precision: 0.9733 - recall: 0.8673 - f1_score: 0.9170 - val_loss: 0.9091 - val_accuracy: 0.8284 - val_precision: 0.8948 - val_recall: 0.8144 - val_f1_score: 0.8527\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.3106 - accuracy: 0.9213 - precision: 0.9747 - recall: 0.8717 - f1_score: 0.9203 - val_loss: 0.9148 - val_accuracy: 0.8300 - val_precision: 0.8932 - val_recall: 0.8168 - val_f1_score: 0.8533\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.3002 - accuracy: 0.9229 - precision: 0.9744 - recall: 0.8765 - f1_score: 0.9228 - val_loss: 0.9132 - val_accuracy: 0.8300 - val_precision: 0.8921 - val_recall: 0.8191 - val_f1_score: 0.8540\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.2893 - accuracy: 0.9268 - precision: 0.9750 - recall: 0.8802 - f1_score: 0.9251 - val_loss: 0.9245 - val_accuracy: 0.8293 - val_precision: 0.8904 - val_recall: 0.8203 - val_f1_score: 0.8539\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.2792 - accuracy: 0.9297 - precision: 0.9745 - recall: 0.8844 - f1_score: 0.9273 - val_loss: 0.9284 - val_accuracy: 0.8304 - val_precision: 0.8915 - val_recall: 0.8203 - val_f1_score: 0.8544\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2694 - accuracy: 0.9322 - precision: 0.9764 - recall: 0.8878 - f1_score: 0.9300 - val_loss: 0.9357 - val_accuracy: 0.8284 - val_precision: 0.8892 - val_recall: 0.8214 - val_f1_score: 0.8539\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.2599 - accuracy: 0.9331 - precision: 0.9755 - recall: 0.8923 - f1_score: 0.9320 - val_loss: 0.9414 - val_accuracy: 0.8304 - val_precision: 0.8878 - val_recall: 0.8203 - val_f1_score: 0.8527\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_sequences_padded, \n",
    "    train_labels, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(val_sequences_padded, valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cee78965-9151-4d3f-90e1-5c3fd66d8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tokenizer is your trained tokenizer\n",
    "with open('../data/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb600c-e180-4abf-a7bc-1186f9bd6efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da47bb-742c-483a-beef-1ddf63e5e245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
