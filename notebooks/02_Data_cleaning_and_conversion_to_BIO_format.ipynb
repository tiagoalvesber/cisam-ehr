{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb90a9b1-89a4-48ad-84de-7b489c84baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4102a988-3ccf-48d7-8bdf-ade5f77b1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tiagolima/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad7cd0f-6db1-4fc1-9815-3a1f8353f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/annotated_json_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11df92c5-9d4d-4eb3-9a2e-d76316bfd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import entity_to_acronyms, acronyms_to_entities, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b9b5ab-b90c-4700-812f-170357cfd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file for reading\n",
    "with open(os.path.join(data_dir, \"annotated_data.json\"), 'r') as f:\n",
    "\n",
    "    # Load the JSON data into a dictionary\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f77893ed-321a-4053-8b4f-d045e9f7c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_punctuation(token):\n",
    "    \"\"\"\n",
    "    Removes trailing punctuation from a token.\n",
    "\n",
    "    Args:\n",
    "        token (str): A string representing the token to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned token with trailing punctuation removed.\n",
    "    \"\"\"\n",
    "    while token and re.search(r'[^\\w\\s\\']', token[-1]):\n",
    "        token = token[:-1]\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db90f7-e2fd-4303-8f87-981ff278a439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29307c13-c32b-4faf-a26f-82cefa3d7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "\n",
    "    regex_match = r'[^\\s\\u200a\\-\\u2010-\\u2015\\u2212\\uff0d]+'  # r'[^\\s\\u200a\\-\\—\\–]+'\n",
    "\n",
    "    tokens = []\n",
    "    start_end_ranges = []\n",
    "\n",
    "    sentence_breaks = []\n",
    "\n",
    "    start_idx = 0\n",
    "    \n",
    "    for sentence in text.split('\\n'):\n",
    "        words = [match.group(0) for match in re.finditer(regex_match, sentence)]\n",
    "        processed_words = list(map(remove_trailing_punctuation, words))\n",
    "        sentence_indices = [(match.start(), match.start() + len(token)) for match, token in\n",
    "                            zip(re.finditer(regex_match, sentence), processed_words)]\n",
    "\n",
    "        # Update the indices to account for the current sentence's position in the entire text\n",
    "        sentence_indices = [(start_idx + start, start_idx + end) for start, end in sentence_indices]\n",
    "\n",
    "        start_end_ranges.extend(sentence_indices)\n",
    "        tokens.extend(processed_words)\n",
    "\n",
    "        sentence_breaks.append(len(tokens))\n",
    "\n",
    "        start_idx += len(sentence) + 1\n",
    "        # print(\"sentence, \".upper(),sentence)\n",
    "    return tokens, start_end_ranges, sentence_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aed921e-6bb3-4267-b208-26682acf13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_id, doc in data.items():\n",
    "#     print(f\"text: {doc['text'][:200]} split: {split_text(doc['text'][:200])} \\n {'*' * 30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4468b6-b93f-4236-9c2d-3f32350ac31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_token(tokens, tags, token_pos, entity):\n",
    "    \"\"\"\n",
    "    Modifies a list of tags by adding a tag label to a token at a given position in the list, based on the position of the \n",
    "    previous token and whether the current token has the same tag label as the previous token.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list): A list of tokens in a sequence.\n",
    "    - tags (list): A list of tag labels corresponding to the tokens in a sequence.\n",
    "    - token_pos (int): The position of the token to tag.\n",
    "    - entity (str): The tag label to add to the token.\n",
    "\n",
    "    Returns:\n",
    "    - tags (list): The modified list of tag labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    tag = entity_to_acronyms[entity]\n",
    "    # print(f\"tokens[token_pos] {tokens[token_pos]}\\ttags[token_pos - 1]: {tags[token_pos - 1]}\\ttokens[token_pos] not in stop_words: {tokens[token_pos] not in stop_words}\")\n",
    "    if token_pos > 0 and f'{tag}' in tags[token_pos - 1]:        \n",
    "        tags[token_pos] = f'I-{tag}'\n",
    "        # print(f\"\\t\\t\\t tag_token 01 tag: {tag} token_pos: {token_pos} tags[token_pos - 1]: {tags[token_pos - 1]} result: {tags[token_pos]}\")\n",
    "    elif tokens[token_pos] not in stop_words:\n",
    "        tags[token_pos] = f'B-{tag}'\n",
    "        # print(f\"\\t\\t\\t tag_token 02 tag: {tag} tokens[token_pos]: {tokens[token_pos]} not stop_words: {tokens[token_pos] not in stop_words} result: {tags[token_pos]}\")\n",
    "    else:\n",
    "        pass\n",
    "        # print(f\"\\t\\t\\t tag_token 03 tag: {tag} tokens[token_pos]: {tokens[token_pos]} result: {tags[token_pos]}\")  \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ebefa61-39c6-40d3-8e31-1bdbcdd9771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bio_files(output_file_path, tokens, tags, sentence_breaks):\n",
    "\n",
    "    # Write the tags to a .bio file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i].strip()\n",
    "            if token:\n",
    "                if i in sentence_breaks:\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(f\"{tokens[i]}\\t{tags[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd59ea8-053c-4ec1-be49-9765ed5772ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ann_to_bio(data, output_dir, filtered_entities=[]):\n",
    "    summary = {}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert annotations from a dictionary of text files to a BIO-tagged sequence.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary of text files where keys are file IDs and values are dictionaries containing 'text' and\n",
    "            'annotations' keys.\n",
    "        filtered_entities (list): A list of entity labels to include. If provided, only annotations with these labels will\n",
    "            be converted to the BIO format. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists: tokens and tags.\n",
    "        - tokens (list): A list of tokens in a sequence.\n",
    "        - tags (list): A list of corresponding tags for each token in the sequence. Tags are BIO formatted.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        # Delete the contents of the directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    # Recreate the directory\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "    for file_id in data:\n",
    "        text = data[file_id]['text']\n",
    "        annotations = data[file_id]['annotations']\n",
    "\n",
    "      \n",
    "        # print(text)\n",
    "        # Tokenizing\n",
    "        tokens, token2text, sentence_breaks = split_text(text)\n",
    "\n",
    "        # Initialize the tags\n",
    "        tags = ['O'] * len(tokens)\n",
    "\n",
    "        ann_pos = 0\n",
    "        token_pos = 0\n",
    "        # for dd,dda in zip(tokens, token2text):\n",
    "        #     print(dd,dda)\n",
    "        # print(\"=\" * 50)\n",
    "        while ann_pos < len(annotations) and token_pos < len(tokens):\n",
    "\n",
    "            label = annotations[ann_pos]['label']\n",
    "            start = annotations[ann_pos]['start']\n",
    "            end = annotations[ann_pos]['end']\n",
    "            # print(f\"label: {label}\\tstart: {start}\\tend:{end}\")\n",
    "            if filtered_entities:\n",
    "                if label not in filtered_entities:\n",
    "                    # increment to access next annotation\n",
    "                    ann_pos += 1\n",
    "                    continue\n",
    "            \n",
    "            ann_word = text[start:end]\n",
    "            # print(f\"\\tann_word: {ann_word}\")\n",
    "            \n",
    "            # find the next word that fall between the annotation start and end\n",
    "            while token_pos < len(tokens) and token2text[token_pos][0] < start:\n",
    "                token_pos += 1\n",
    "            \n",
    "            if tokens[token_pos] == ann_word or ann_word in tokens[token_pos] or re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos]):\n",
    "                tag_token(tokens, tags, token_pos, label)\n",
    "                # print(f\"\\t\\tRULE_01 tk[token_pos]: {tokens[token_pos]}\")\n",
    "            elif ann_word in tokens[token_pos - 1] or  ann_word in tokens[token_pos - 1] or re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos - 1]):\n",
    "                tag_token(tokens, tags, token_pos - 1, label)\n",
    "                # print(f\"\\t\\tRULE_02 tk[token_pos]: {tokens[token_pos]}\")\n",
    "            else:\n",
    "                pass\n",
    "                # print(tokens[token_pos], tokens[token_pos - 1], ann_word, label)\n",
    "            # print(\"\\t\\t\",tokens[token_pos], tokens[token_pos - 1], ann_word, label)\n",
    "            # increment to access next annotation\n",
    "            ann_pos += 1\n",
    "\n",
    "        # break\n",
    "        # write to bio file\n",
    "        # print(\"*\" * 50)\n",
    "        write_bio_files(os.path.join(output_dir, f\"{file_id}.bio\"), tokens, tags, sentence_breaks)\n",
    "    print(\"Conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e570b91-3fec-44c1-a731-e17b5a07854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete\n"
     ]
    }
   ],
   "source": [
    "output_dir='../data/bio_json_data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir,  exist_ok=True, mode=0o777)\n",
    "\n",
    "# convert_ann_to_bio(data, output_dir=output_dir, filtered_entities = []) \n",
    "# convert_ann_to_bio(data, output_dir=output_dir, filtered_entities = ['Age','Therapeutic_procedure', 'Date','Time']) \n",
    "#                                                               # 'Lab_value', 'Medication', 'Color', 'Age',\n",
    "#                                                               # 'Clinical_event', 'Pregnancy', 'Pregnancy_history',\n",
    "#                                                               # 'Therapeutic_procedure', 'Date',\n",
    "#                                                               # 'Subject'])\n",
    "convert_ann_to_bio(data, output_dir=output_dir\n",
    "                   , filtered_entities=[\n",
    "                                        'Sign_symptom',  'Diagnostic_procedure',\n",
    "                                        'Lab_value' ,'Disease_disorder',\n",
    "                                        'Pregnancy', 'Pregnancy_history',\n",
    "                                        'Color', 'Subject', \n",
    "                                        'Age','Clinical_event',\n",
    "                                        'Date', 'Time',\n",
    "                                        'Therapeutic_procedure','Medication',\n",
    "                                        'Biological_attribute'\n",
    "                                       ]) \n",
    "                                                                  # 'Lab_value', 'Medication', 'Color', 'Age',\n",
    "                                                                  # 'Clinical_event', 'Pregnancy', 'Pregnancy_history',\n",
    "                                                                  # 'Therapeutic_procedure', 'Date',\n",
    "                                                                  # 'Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416ffde-44a8-45c1-9002-6358143c589f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c283d-294a-4f62-a025-7dd2f1cb2228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e3063-9385-4ee8-8d8e-baf5ee4fe34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802542ec-5227-4e6c-a89c-e1b75f4af072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
